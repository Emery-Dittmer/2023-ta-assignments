---
title: "Assignment 3"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Pre-Processing

## Loading Documents and Basic Packages

```{r load-data}
#personal lveve daa from correction
person_level_data= read.csv('person_level_data.csv')
#data of art unitu changes
aus=read.csv("examiner_aus.csv")
#get id crosswalk table
ids=read.csv("examiner_ids.csv")


#load packages
library(dplyr)
```

## Pre-Processing Data for Aggregation by year

```{r aggregate the examiner data by year and by departement}

#cleaning data for ids.Remove Nas
ids <- ids %>% 
  filter(!is.na(old_pid))

#remove duplicatse in ids
ids <- ids %>% 
  distinct(
    old_pid,
    .keep_all = TRUE
)

#add the number of changes to art unit
aus_changes <- aus %>%
  group_by(old_pid) %>% 
  summarise(
    art_unit_changes = n(),
    art_unit_distinct_changes = n_distinct(examiner_art_unit),
  )

#merge the ids and aus changes together so we can then merge this with the person level data
aus_changes <-merge(
  aus_changes,
  ids,
  by='old_pid'
)

#rename patex_id to examiner_id for consistency
aus_changes <- aus_changes %>%
  rename('examiner_id'='patex_id')


person_level_data_complete <- merge(
  person_level_data,
  aus_changes,
  by='examiner_id',
  all.x= T
)

#remove the NAs from the art_unit changes in dataset
person_level_data_complete <- person_level_data_complete %>%
filter(!is.na(art_unit_changes))

```
## Feature Engineering for Categorical Variable

```{r engineer binary variable }
#if the employee is above the average number of art unit changes then they are a 1 else they are a 0 

#first create annual changes
person_level_data_complete$annual_art_change <- with(person_level_data_complete, art_unit_changes/365)

mean_art_changes = mean(person_level_data_complete$annual_art_change)

person_level_data_complete$high_mobility <- ifelse(person_level_data_complete$annual_art_change > mean_art_changes ,"Yes","No")

person_level_data_complete$high_mobility<-as.factor(person_level_data_complete$high_mobility)


#remove the un-needed columns for testing dataset
drop <- c("old_pid","examiner_name","examiner_id","art_unit_changes","art_unit_changes_distinct","new_pid","annual_art_change")
person_level_data_complete = person_level_data_complete[,!(names(person_level_data_complete) %in% drop)]



```


# Analysis of data using Logistic Regression and Decision Trees

## Evaluation of Strength of Logistic and Tree Models
### Assume all Data Used for Training and Testing
#### Logistic regression

```{r logistic regression with grm all data}
predictors<-c("art_unit","gender","start_year","tenure_days","tc","work_group","art_unit_distinct_changes")
logit=glm(high_mobility~art_unit+gender+start_year+tenure_days+tc+work_group+art_unit_distinct_changes,data=person_level_data_complete,family = "binomial")


#logit_complete=glm(high_mobility~art_unit+gender+start_year+latest_date+tenure_days+tc+work_group+art_unit_distinct_changes,data=person_level_data_complete,family = "binomial")

summary(logit)

```

```{r logistic regression with lrm all data}
library(rms)
logit_lrm=lrm(high_mobility~art_unit+gender+start_year+tenure_days+tc+work_group+art_unit_distinct_changes,data=person_level_data_complete)


#logit_complete=glm(high_mobility~art_unit+gender+start_year+latest_date+tenure_days+tc+work_group+art_unit_distinct_changes,data=person_level_data_complete,family = "binomial")

logit_lrm


```
#### Tree model

```{r tree model all data}
library(tree)
library(rpart)
library(rpart.plot)

mytree=rpart(high_mobility~art_unit+gender+start_year+tenure_days+tc+work_group+art_unit_distinct_changes,data=person_level_data_complete,control=rpart.control(cp=0.01))
rpart.plot(mytree)



```

```{r optimal cp value for tree all data}
myoverfittedtree=rpart(high_mobility~art_unit+gender+start_year+tenure_days+tc+work_group+art_unit_distinct_changes,data=person_level_data_complete,control=rpart.control(cp=0.0001))
opt_cp=myoverfittedtree$cptable[which.min(myoverfittedtree$cptable[,"xerror"]),"CP"]
plotcp(myoverfittedtree)
```

```{r optimal tree all data}
mytree_optimal=rpart(high_mobility~art_unit+gender+start_year+tenure_days+tc+work_group+art_unit_distinct_changes,data=person_level_data_complete,control=rpart.control(cp=opt_cp))
rpart.plot(mytree_optimal)

remove(mytree,myoverfittedtree)
       
```

```{r Strenght evaluation all data}
##Accuarcy evaluation with caret
#install.packages("caret")
require(caret) 

##set up variables
#logistic regression glm
y_logit = predict(logit,person_level_data_complete)
#logistic regression rlm
y_logit_lrm = predict(logit_lrm,person_level_data_complete)
#prediction with tree
y_tree= predict(mytree_optimal,person_level_data_complete)
y_tree<-as.data.frame(y_tree)
y_tree$pred<- ifelse(y_tree$Yes > 0.5,"Yes","No")


###logsitic regression
person_level_data_complete$glm_prediction <- y_logit
person_level_data_complete$glm_prediction <- ifelse(person_level_data_complete$glm_prediction > 0.5,"Yes","No")
person_level_data_complete$glm_prediction <-as.factor(person_level_data_complete$glm_prediction)


cm<-confusionMatrix(
  data=person_level_data_complete$glm_prediction, #data is the prediction
  reference=person_level_data_complete$high_mobility) #reference is the 'true' value
accuracy_glm=cm$overall[1]
precision_glm=cm$byClass[5]
recall_glm=cm$byClass[6]
F1_glm=cm$byClass[7]
  


#logistic regression rlm
person_level_data_complete$rlm_prediction <- y_logit_lrm
person_level_data_complete$rlm_prediction <- ifelse(person_level_data_complete$rlm_prediction > 0.5,"Yes","No")
person_level_data_complete$rlm_prediction <-as.factor(person_level_data_complete$rlm_prediction)

cm<-confusionMatrix(
  data=person_level_data_complete$rlm_prediction, #data is the prediction
  reference=person_level_data_complete$high_mobility) #reference is the 'true' value
accuracy_rlm=cm$overall[1]
precision_rlm=cm$byClass[5]
recall_rlm=cm$byClass[6]
F1_rlm=cm$byClass[7]
  

#tree regression
person_level_data_complete$tree_prediction <- y_tree$pred
#person_level_data_complete$tree_acc <- ifelse(person_level_data_complete$tree_prediction > 0.5,"Yes","No")
person_level_data_complete$tree_prediction <- as.factor(person_level_data_complete$tree_prediction)

levels(person_level_data_complete$high_mobility)
levels(person_level_data_complete$tree_prediction)
   
cm<-confusionMatrix(
  data=person_level_data_complete$tree_prediction, #data is the prediction
  reference=person_level_data_complete$high_mobility) #reference is the 'true' value
accuracy_tree=cm$overall[1]
precision_tree=cm$byClass[5]
recall_tree=cm$byClass[6]
F1_tree=cm$byClass[7]



#drop=c('glm_prediction','rlm_prediction','tree_prediction')
#person_level_data_complete = person_level_data_complete[,!(names(person_level_data_complete) %in% drop)]

#remove redundant data
remove(y_tree,cm,y_logit_lrm,y_logit_glm,y_logit)

```

#### Accuracy of Models with all Data

```{r results print out all data}

c(accuracy_glm,precision_glm,recall_glm,F1_glm)

c(accuracy_rlm,precision_rlm,recall_rlm,F1_rlm)

c(accuracy_tree,precision_tree,recall_tree,F1_tree)

```

### Assume Data Split into Training and Testing data sets

```{r Train & Test}
#make this example reproducible
set.seed(1)

#use 70% of dataset as training set and 30% as test set
sample <- sample(c(TRUE, FALSE), nrow(person_level_data_complete), replace=TRUE, prob=c(0.7,0.3))
train  <- person_level_data_complete[sample, ]
test   <- person_level_data_complete[!sample, ]

```

#### Logistic regression

```{r logistic regression with grm Train & Test}
logit=glm(
  high_mobility~art_unit+gender+start_year+tenure_days+tc+work_group+art_unit_distinct_changes,
  data=train,
  family = "binomial"
)

summary(logit)

```

#### Tree model

```{r optimal cp value for tree Train & Test}
myoverfittedtree=rpart(
  high_mobility~art_unit+gender+start_year+tenure_days+tc+work_group+art_unit_distinct_changes,
  data=train,
  control=rpart.control(cp=0.0001)
)
opt_cp=myoverfittedtree$cptable[which.min(myoverfittedtree$cptable[,"xerror"]),"CP"]
plotcp(myoverfittedtree)
remove(myoverfittedtree)
```

```{r optimal tree Train & Test}
mytree_optimal=rpart(
  high_mobility~art_unit+gender+start_year+tenure_days+tc+work_group+art_unit_distinct_changes,
  data=person_level_data_complete,
  control=rpart.control(cp=opt_cp)
)
rpart.plot(mytree_optimal)
```

```{r Strenght evaluation Train & Test}
##Accuarcy evaluation with caret
#install.packages("caret")
require(caret) 

##set up variables within the same dataframe
#logistic regression glm
y_logit = predict(logit,test)
#prediction with tree
y_tree= predict(mytree_optimal,test)
y_tree<-as.data.frame(y_tree)
y_tree$pred<- ifelse(y_tree$Yes > 0.5,"Yes","No")


###logsitic regression
test$glm_prediction <- y_logit
test$glm_prediction <- ifelse(test$glm_prediction > 0.5,"Yes","No")
test$glm_prediction <-as.factor(test$glm_prediction)


cm<-confusionMatrix(
  data=test$glm_prediction, #data is the prediction
  reference=test$high_mobility) #reference is the 'true' value
accuracy_glm=cm$overall[1]
precision_glm=cm$byClass[5]
recall_glm=cm$byClass[6]
F1_glm=cm$byClass[7]
  

#tree regression
test$tree_prediction <- y_tree$pred
#person_level_data_complete$tree_acc <- ifelse(person_level_data_complete$tree_prediction > 0.5,"Yes","No")
test$tree_prediction <- as.factor(test$tree_prediction)

cm<-confusionMatrix(
  data=test$tree_prediction, #data is the prediction
  reference=test$high_mobility) #reference is the 'true' value
accuracy_tree=cm$overall[1]
precision_tree=cm$byClass[5]
recall_tree=cm$byClass[6]
F1_tree=cm$byClass[7]

#remove redundant data
remove(y_tree,cm,y_logit)

```

#### Accuracy of Training & Test Models

```{r results print out Train & Test}

c(accuracy_glm,precision_glm,recall_glm,F1_glm)

c(accuracy_tree,precision_tree,recall_tree,F1_tree)
```

# ROC Curves & AUC with Training & Test Data

## ROC &  Precision-Recall Curves

### Logistic Regression 

```{r ROC curve}
#install.packages("precrec")
library(precrec)
##note for SScurves
#Scores are predictions
#labels are observed

high_mobility_labels <- as.vector(test$high_mobility)
high_mobility_labels <- ifelse(high_mobility_labels=="Yes",1,0)
#glm
#vectorize and set to binary
glm_prediction <- as.vector(test$glm_prediction)
glm_prediction <- ifelse(glm_prediction=="Yes",1,0)

sscurves_glm <- evalmod(scores = glm_prediction, labels = high_mobility_labels)

# Show ROC and Precision-Recall plots
plot(sscurves_glm)

#create auc for glm
aucs_glm <- auc(sscurves_glm)

```
```{r glm precesion recall plots}
# Show a Precision-Recall plot
plot(sscurves_glm, "PRC")
```



### Tree Model

```{r Tree ROC }
#glm
#vectorize and set to binary
tree_prediction <- as.vector(test$tree_prediction)
tree_prediction <- ifelse(tree_prediction=="Yes",1,0)

sscurves_tree <- evalmod(scores = tree_prediction, labels = high_mobility_labels)

# Show ROC and Precision-Recall plots
plot(sscurves_tree)

#create auc for tree
aucs_tree <- auc(sscurves_tree)

```

```{r tree precesion recall plots}
# Show a Precision-Recall plot
plot(sscurves_tree, "PRC")
```


## AUC for best curve
### Logistic Regression
```{r AUC GLM }
# Get a data frame with AUC scores
aucs_glm
# Use knitr::kable to display the result in a table format
#knitr::kable(aucs)
```

### Tree Model
```{r AUC Tree }
# Get a data frame with AUC scores
aucs_tree
# Use knitr::kable to display the result in a table format
#knitr::kable(aucs)
```


## Unused code Section


#### Lrm Model
```{r RLM ROC }
# #rlm
# #vectorize and set to binary
# rlm_prediction <- as.vector(person_level_data_complete$rlm_prediction)
# rlm_prediction <- ifelse(rlm_prediction=="Yes",1,0)
# 
# sscurves_rlm <- evalmod(scores = rlm_prediction, labels = high_mobility_labels)
# 
# # Show ROC and Precision-Recall plots
# plot(sscurves_rlm)
# 
# #create auc for rlm
# aucs_rlm <- auc(sscurves_rlm)
```

```{r rlm precesion recall plots}
# # Show a Precision-Recall plot
# plot(sscurves_rlm, "PRC")
```

